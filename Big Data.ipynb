{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c0ca0c",
   "metadata": {},
   "source": [
    "Here's a rough sketch of how you might implement the Perceptron algorithm in PySpark.\n",
    "Please note that you may need to adjust the following script to fit your actual application environment and data format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db29c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import rand\n",
    "# Initialize w\n",
    "w=[0,0]\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "# Initialize w\n",
    "w = [0.0, 0.0]\n",
    "# Split data into training, validation, and test sets\n",
    "df = df.withColumn(\"rand\", rand())\n",
    "training_data = df.filter(df[\"rand\"] < 1/3)\n",
    "validation_data = df.filter((df[\"rand\"] >= 1/3) & (df[\"rand\"] < 2/3))\n",
    "test_data = df.filter(df[\"rand\"] >= 2/3)\n",
    "# Function to compute prediction\n",
    "def predict(row):\n",
    "    return 1 if w[0]*row[\"features\"][0] + w[1]*row[\"features\"][1] >= 0 else -1\n",
    "# Function to update weights\n",
    "def update_weights(data, alpha):\n",
    "    global w\n",
    "    for row in data.collect():\n",
    "        error = row[\"target\"] - predict(row)\n",
    "        w[0] += alpha * error * row[\"features\"][0]\n",
    "        w[1] += alpha * error * row[\"features\"][1]\n",
    "# Function to compute accuracy\n",
    "def compute_accuracy(data):\n",
    "    correct_predictions = data.rdd.filter(lambda row: row[\"target\"] == predict(row)).count()\n",
    "    total_predictions = data.count()\n",
    "    return correct_predictions / total_predictions\n",
    "# Train perceptron models\n",
    "for alpha in [1, 2]:\n",
    "    # Reset w\n",
    "    w = [0.0, 0.0]\n",
    "    # Train\n",
    "    for _ in range(T):\n",
    "        update_weights(training_data, alpha)\n",
    "    # Validate\n",
    "    validation_accuracy = compute_accuracy(validation_data)\n",
    "    print(f\"Validation accuracy for alpha={alpha}: {validation_accuracy}\")\n",
    "# Here, I assume you selected alpha = 1, but in practice you would select the alpha that gave the best validation accuracy\n",
    "w = [0.0, 0.0]\n",
    "for _ in range(T):\n",
    "    update_weights(training_data, 1)\n",
    "# Test\n",
    "test_accuracy = compute_accuracy(test_data)\n",
    "print(f\"Test accuracy (generalization accuracy): {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e932c25",
   "metadata": {},
   "source": [
    "\n",
    "Please note that this is a very simple implementation and assumes that the data fits in memory. \n",
    "In practice, you would want to use a more sophisticated approach (e.g., mini-batch gradient descent) for large datasets. Additionally, you would need to adjust this script based on the actual format of your data. In this script, I assumed that the dataframe has a \"features\" column that contains a list of two features, and a \"target\" column that contains the target label.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
